---
phase: 08-monitoring-and-observability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - spec/13-monitoring-observability.md
autonomous: true

must_haves:
  truths:
    - "The spec defines a structured JSON log format for 12 FL training event types with specific fields per event"
    - "The spec defines 11 Prometheus metric definitions for Flower training with types, labels, and descriptions"
    - "The spec defines DCGM Exporter as sidecar container on SuperNode for GPU metrics with specific DCGM metric names"
    - "The spec includes 3 Grafana dashboard definitions with specific panels and PromQL queries"
    - "The spec defines 8 alerting rules (4 FL training + 4 GPU health) with PromQL expressions and severity levels"
    - "The spec defines the monitoring architecture where appliances run exporters and monitoring infrastructure is operator-managed"
  artifacts:
    - path: "spec/13-monitoring-observability.md"
      provides: "Complete monitoring and observability specification covering OBS-01 and OBS-02"
      min_lines: 500
      contains: "OBS-01"
  key_links:
    - from: "spec/13-monitoring-observability.md"
      to: "spec/10-gpu-passthrough.md"
      via: "DCGM Exporter requires FL_GPU_ENABLED=YES from Phase 6"
      pattern: "FL_GPU_ENABLED"
    - from: "spec/13-monitoring-observability.md"
      to: "spec/09-training-configuration.md"
      via: "Metrics exporter embedded in ServerApp leverages strategy/checkpoint events from Phase 5"
      pattern: "ServerApp"
    - from: "spec/13-monitoring-observability.md"
      to: "spec/03-contextualization-reference.md"
      via: "New CONTEXT variables FL_METRICS_ENABLED, FL_METRICS_PORT, FL_DCGM_ENABLED, FL_LOG_FORMAT"
      pattern: "FL_METRICS_ENABLED"
---

<objective>
Create the complete monitoring and observability specification (spec/13-monitoring-observability.md) covering both OBS-01 (structured JSON logging for FL training events, zero infrastructure) and OBS-02 (Prometheus/Grafana stack with custom Flower metrics exporter, DCGM Exporter for GPU, pre-built dashboards, and alerting rules).

Purpose: This spec enables operators to gain visibility into FL training progress, GPU utilization, and cluster health -- essential for production deployments where training jobs may run for hours or days.

Output: spec/13-monitoring-observability.md -- a self-contained spec section satisfying requirements OBS-01 and OBS-02 and all 4 phase success criteria.
</objective>

<execution_context>
@/home/pablo/.claude/get-shit-done/workflows/execute-plan.md
@/home/pablo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-monitoring-and-observability/08-RESEARCH.md
@spec/01-superlink-appliance.md
@spec/02-supernode-appliance.md
@spec/09-training-configuration.md
@spec/10-gpu-passthrough.md
@spec/03-contextualization-reference.md
@spec/00-overview.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create spec/13-monitoring-observability.md -- OBS-01 Structured Logging and OBS-02 Architecture (Sections 1-7)</name>
  <files>spec/13-monitoring-observability.md</files>
  <action>
Create spec/13-monitoring-observability.md with the standard spec header:
- Requirements: OBS-01, OBS-02
- Phase: 08 - Monitoring and Observability
- Status: Specification

Write sections 1-7:

**Section 1: Purpose and Scope.** Define the two-tier monitoring approach:
- Tier 1 (OBS-01): Structured JSON logging for FL training events. Zero additional infrastructure. Works standalone.
- Tier 2 (OBS-02): Full Prometheus/Grafana stack with custom Flower metrics exporter, DCGM Exporter for GPU metrics, pre-built dashboards, and alerting rules. Builds on Tier 1. Requires operator-managed monitoring infrastructure.
- Key architectural principle: Appliances run EXPORTERS (data sources). Monitoring infrastructure (Prometheus, Grafana, Alertmanager) is operator-managed and NOT embedded in the appliance VMs.
- Cross-references: spec/01 (SuperLink boot), spec/02 (SuperNode boot), spec/09 (training configuration for strategy/checkpoint events), spec/10 (GPU passthrough for DCGM), spec/03 (contextualization reference for new variables).
- What this spec does NOT cover: log aggregation (Loki), host-level metrics (Node Exporter), container metrics (cAdvisor). These are optional operator additions mentioned but not specified.

**Section 2: Monitoring Architecture Diagram.** ASCII diagram showing:
- Monitoring Infrastructure box (operator-managed): Prometheus (:9090), Grafana (:3000), Alertmanager (:9093 -- note: this is NOT Flower's Control API 9093, it's on a separate VM).
- SuperLink VM: flower-superlink container (stdout JSON logs) + fl-metrics-exporter (port 9101, /metrics endpoint).
- SuperNode VM (GPU): flower-supernode container (stdout JSON logs) + dcgm-exporter sidecar (port 9400, /metrics endpoint).
- Arrows: Prometheus scrapes 9101 and 9400. Grafana reads from Prometheus. Alertmanager receives alert rules from Prometheus.
- Port allocation table showing all ports and their services to avoid conflicts:
  | Port | Service | VM | Notes |
  | 9090 | Prometheus | Monitoring (operator) | NOT on appliance VMs |
  | 9091 | ServerAppIo (Flower) | SuperLink | Existing Phase 1 |
  | 9092 | Fleet API (Flower) | SuperLink | Existing Phase 1 |
  | 9093 | Control API (Flower) | SuperLink | Existing Phase 1 |
  | 9101 | FL metrics exporter | SuperLink | New Phase 8 |
  | 9400 | DCGM Exporter | SuperNode | New Phase 8 |
  | 3000 | Grafana | Monitoring (operator) | NOT on appliance VMs |

**Section 3: Tier 1 -- Structured JSON Logging (OBS-01).** Cover:
- JSON log format specification: single-line JSON with fields: timestamp (ISO 8601 UTC), level, logger ("flwr"), role ("superlink" or "supernode"), event (FL event type), data (event-specific dict), message, source (file:line).
- Include a concrete example JSON log line.
- FL Event Taxonomy table with all 12 event types:
  | Event Type | Level | Trigger | Data Fields |
  | training_start | INFO | ServerApp main() begins | strategy, num_rounds, min_clients |
  | round_start | INFO | Strategy selects clients | round, num_clients_selected, strategy |
  | round_end | INFO | Aggregation complete | round, num_clients_responded, aggregated_loss, aggregated_accuracy, round_duration_seconds |
  | training_end | INFO | All rounds complete | total_rounds, final_loss, final_accuracy, total_duration_seconds |
  | client_join | INFO | SuperNode connects | node_id, client_address |
  | client_leave | INFO | SuperNode disconnects | node_id, reason |
  | checkpoint_saved | INFO | Checkpoint written | round, path, size_bytes |
  | evaluation_result | INFO | evaluate_fn returns | round, loss, accuracy, num_examples |
  | client_failure | WARNING | Client fails fit/evaluate | round, node_id, error_type |
  | training_stalled | WARNING | Round timeout | round, waiting_since_seconds, connected_clients, required_clients |
  | gpu_detected | INFO | GPU validation at boot | gpu_name, gpu_memory_mb, driver_version |
  | gpu_unavailable | WARNING | GPU requested but absent | fallback_device |
- Implementation approach: FlowerJSONFormatter class replaces Flower's default text handler formatter. Constructed from raw LogRecord attributes (not the pre-formatted string). Activated when FL_LOG_FORMAT=json.
- Include the FlowerJSONFormatter reference implementation (Python class from research).
- Include the log_fl_event() helper function (from research).
- Docker log capture: Flower container writes to stdout. Docker json-file log driver captures with timestamps and rotation. No additional log infrastructure needed.
- Log rotation: Docker default (100MB max, 1 file). Operator can configure docker daemon.json for different rotation policy.

**Section 4: New Contextualization Variables.** Define all 4 new variables introduced by Phase 8:
- FL_LOG_FORMAT: O|list|Log output format|text,json|text -- Service-level (OneFlow), applies to both appliances. Switches between Flower's default text format and structured JSON. When "json", the FlowerJSONFormatter replaces the default handler formatter on the "flwr" logger.
- FL_METRICS_ENABLED: O|boolean|Enable Prometheus metrics exporter||NO -- SuperLink only. Master switch for OBS-02 Prometheus metrics. When YES, starts prometheus_client HTTP server on FL_METRICS_PORT. The ServerApp FAB must include prometheus_client in its dependencies.
- FL_METRICS_PORT: O|number|Prometheus metrics exporter port||9101 -- SuperLink only. Port for the FL training metrics HTTP endpoint. Must be in range 1024-65535. Must not conflict with 9091-9093 (Flower ports).
- FL_DCGM_ENABLED: O|boolean|Enable DCGM GPU metrics exporter||NO -- SuperNode only. Master switch for DCGM Exporter sidecar container. Requires FL_GPU_ENABLED=YES (Phase 6). If GPU not available, DCGM is not started (warning logged).
- Variable interaction matrix: table showing which variables enable which tier and which appliance they apply to.
- Include USER_INPUT definitions in copy-paste ready format.

**Section 5: Tier 2 -- Flower Training Metrics Exporter (OBS-02).** Cover:
- Purpose: expose FL training metrics to Prometheus via HTTP endpoint on SuperLink.
- Implementation: prometheus_client library embedded in ServerApp FAB code. NOT added to base Flower Docker image.
- Metric definitions table (all 11 metrics from research):
  | Metric Name | Type | Labels | Description |
  | fl_round_current | Gauge | strategy | Current training round |
  | fl_round_total | Gauge | strategy | Total configured rounds |
  | fl_round_duration_seconds | Histogram | strategy | Round duration (buckets: 1,5,10,30,60,120,300,600) |
  | fl_aggregated_loss | Gauge | strategy | Loss after latest round |
  | fl_aggregated_accuracy | Gauge | strategy | Accuracy after latest round |
  | fl_clients_connected | Gauge | -- | Connected SuperNodes |
  | fl_clients_selected | Gauge | -- | Clients selected for current round |
  | fl_clients_responded | Gauge | -- | Clients responded in current round |
  | fl_clients_failed | Counter | -- | Total client failures |
  | fl_checkpoint_saved_total | Counter | -- | Total checkpoints saved |
  | fl_training_status | Gauge | -- | Status: 0=idle, 1=running, 2=complete, 3=failed |
- CRITICAL: Label cardinality rules. Do NOT use "round" as a label (creates unbounded time series). Use single gauges updated in-place; Prometheus time-series collection tracks history.
- Include the reference implementation (Python code from research showing metric definitions, start_metrics_server, update_round_metrics).
- Integration point: metrics updated in the ServerApp's evaluate_fn callback and strategy factory, leveraging the Result object's train_metrics_clientapp and evaluate_metrics_clientapp dictionaries.
- Activation: FL_METRICS_ENABLED=YES triggers start_http_server(FL_METRICS_PORT) during ServerApp initialization.

**Section 6: Tier 2 -- DCGM Exporter for GPU Metrics (OBS-02).** Cover:
- Purpose: expose NVIDIA GPU metrics from SuperNode VMs to Prometheus.
- DCGM Exporter container: nvcr.io/nvidia/k8s/dcgm-exporter:4.5.1-4.8.0-distroless. Runs as sidecar alongside flower-supernode container.
- Docker run command with required flags: --gpus all, --cap-add SYS_ADMIN, -p 9400:9400.
- Systemd unit: dcgm-exporter.service (After=docker.service flower-supernode.service, PartOf=flower-supernode.service). Include the complete unit file from research.
- Lifecycle: started when FL_DCGM_ENABLED=YES AND FL_GPU_ENABLED=YES AND GPU detection succeeds. Not started otherwise.
- DCGM image is NOT pre-baked in QCOW2. Pulled at boot time when FL_DCGM_ENABLED=YES (keeps base image size stable). Note: this requires network access at boot when DCGM monitoring is enabled.
- Key DCGM metrics table (8 metrics from research):
  | Metric Name | Type | Description | Alerting Use |
  | DCGM_FI_DEV_GPU_UTIL | Gauge | GPU utilization (%) | Stalled training < 5% |
  | DCGM_FI_DEV_MEM_COPY_UTIL | Gauge | Memory utilization (%) | Memory pressure |
  | DCGM_FI_DEV_FB_FREE | Gauge | Free framebuffer (MiB) | OOM prevention |
  | DCGM_FI_DEV_FB_USED | Gauge | Used framebuffer (MiB) | Memory tracking |
  | DCGM_FI_DEV_GPU_TEMP | Gauge | GPU temperature (C) | Thermal throttling |
  | DCGM_FI_DEV_POWER_USAGE | Gauge | Power draw (W) | Power anomaly |
  | DCGM_FI_DEV_XID_ERRORS | Gauge | Last XID error value | Hardware error |
  | DCGM_FI_DEV_SM_CLOCK | Gauge | SM clock frequency (MHz) | Clock throttling |
- DCGM version compatibility note: Exporter 4.5.1 tested with NVIDIA driver 545+.

**Section 7: Tier 2 -- Prometheus Scrape Configuration (OBS-02).** Cover:
- Prometheus is operator-managed. The spec provides reference configuration, not deployment instructions.
- Scrape configuration snippet (YAML) with two job definitions:
  - fl_training: scrape SuperLink VM port 9101, labels: role=superlink, deployment=flower-opennebula.
  - dcgm_gpu: scrape SuperNode VMs port 9400, labels: role=supernode, deployment=flower-opennebula.
- Scrape interval: 15s (default). Adjustable by operator.
- Service discovery: static_configs as default (operator fills in IP addresses). Note for dynamic environments: file_sd_configs with a script querying OneFlow/OneGate for current VM IPs (not specified in detail -- operator integration).
- Network requirements: Prometheus must be able to reach SuperLink:9101 and SuperNode:9400. Document required port openings for OpenNebula security groups if applicable.
  </action>
  <verify>
Check that the file exists and covers OBS-01 and OBS-02 sections 1-7:
- grep "OBS-01" spec/13-monitoring-observability.md
- grep "OBS-02" spec/13-monitoring-observability.md
- grep "FlowerJSONFormatter" spec/13-monitoring-observability.md
- grep "fl_round_current" spec/13-monitoring-observability.md
- grep "DCGM_FI_DEV_GPU_UTIL" spec/13-monitoring-observability.md
- grep "FL_METRICS_ENABLED" spec/13-monitoring-observability.md
- grep "9101" spec/13-monitoring-observability.md
  </verify>
  <done>Sections 1-7 of spec/13-monitoring-observability.md exist with monitoring architecture, structured logging specification (12 event types), metric definitions (11 FL metrics), DCGM Exporter sidecar, contextualization variables (4 new vars), and Prometheus scrape configuration.</done>
</task>

<task type="auto">
  <name>Task 2: Complete spec/13-monitoring-observability.md -- Grafana Dashboards, Alerting, Anti-Patterns (Sections 8-13)</name>
  <files>spec/13-monitoring-observability.md</files>
  <action>
Append sections 8-13 to the existing spec/13-monitoring-observability.md:

**Section 8: Tier 2 -- Grafana Dashboard Definitions (OBS-02).** Cover:
- Provisioning structure: grafana/provisioning/datasources/prometheus.yaml, grafana/provisioning/dashboards/dashboards.yaml, grafana/dashboards/*.json. Include both provisioning YAML files from the research.
- Dashboard 1: FL Training Overview. Panel table:
  | Panel | Type | PromQL Query | Purpose |
  | Training Progress | Stat | fl_round_current / fl_round_total | Round completion progress |
  | Loss Convergence Curve | Time Series | fl_aggregated_loss | Loss over rounds (convergence) |
  | Accuracy Convergence Curve | Time Series | fl_aggregated_accuracy | Accuracy over rounds |
  | Round Duration | Time Series | rate(fl_round_duration_seconds_sum[5m]) / rate(fl_round_duration_seconds_count[5m]) | Average round duration trend |
  | Client Participation | Time Series | fl_clients_responded | Clients per round |
  | Training Status | Stat | fl_training_status | Status indicator (idle/running/complete/failed) |
- Dashboard 2: GPU Health. Panel table:
  | Panel | Type | PromQL Query | Purpose |
  | GPU Utilization | Time Series | DCGM_FI_DEV_GPU_UTIL | Per-GPU util over time |
  | GPU Memory Usage | Time Series | DCGM_FI_DEV_FB_USED / (DCGM_FI_DEV_FB_USED + DCGM_FI_DEV_FB_FREE) * 100 | Memory % |
  | GPU Temperature | Time Series | DCGM_FI_DEV_GPU_TEMP | Temp with threshold lines |
  | Power Usage | Time Series | DCGM_FI_DEV_POWER_USAGE | Power over time |
  | XID Errors | Stat | DCGM_FI_DEV_XID_ERRORS | Error indicator |
  | Free Memory | Gauge | DCGM_FI_DEV_FB_FREE | Available GPU memory |
- Dashboard 3: Client Health. Panel table:
  | Panel | Type | PromQL Query | Purpose |
  | Connected Clients | Stat | fl_clients_connected | Current count |
  | Client Failures | Time Series | rate(fl_clients_failed[5m]) | Failure rate |
  | Client Participation Rate | Time Series | fl_clients_responded / fl_clients_selected * 100 | Participation % |
  | Client Response Time | Histogram | fl_round_duration_seconds | Round time distribution |
- Note: dashboards are defined as panel specifications in this spec. Operators export them as Grafana JSON using the Grafana UI or API. A reference NVIDIA DCGM dashboard is available at Grafana Dashboard ID 12239.

**Section 9: Tier 2 -- Alerting Rules (OBS-02).** Cover:
- Alerting rules are defined as Prometheus recording/alerting rules. Operators add the YAML to their Prometheus rules_files configuration.
- FL Training Alerts (4 rules). Include the complete YAML from research:
  1. FLTrainingStalled: fl_training_status==1 AND round unchanged for 15m. Severity: critical.
  2. FLExcessiveClientDropout: connected < 50% of selected for 5m. Severity: warning.
  3. FLClientFailureRate: rate(fl_clients_failed[10m]) > 0.5 for 5m. Severity: warning.
  4. FLTrainingNotStarted: status==0 AND rounds configured for 30m. Severity: warning.
- GPU Health Alerts (4 rules). Include the complete YAML from research:
  1. GPUMemoryExhaustion: < 10% free for 2m. Severity: critical.
  2. GPUUtilizationDrop: < 5% util with memory allocated for 10m. Severity: warning.
  3. GPUTemperatureCritical: > 90C for 2m. Severity: critical.
  4. GPUXIDError: XID > 0 for 1m. Severity: critical.
- Alertmanager integration: reference configuration only. Operators configure their own routing (email, Slack, PagerDuty, webhooks). Include a minimal route example pointing to a generic receiver.

**Section 10: Boot Sequence Integration.** Define how monitoring components integrate into existing appliance boot sequences:
- SuperLink boot sequence changes when FL_LOG_FORMAT=json:
  - After Step 4 (validate_config): configure JSON formatter on flwr logger if FL_LOG_FORMAT=json.
  - After Step 10 (start Flower container): start prometheus_client HTTP server if FL_METRICS_ENABLED=YES.
  - No new boot steps -- formatting and metrics server are embedded in existing steps.
- SuperNode boot sequence changes when FL_DCGM_ENABLED=YES:
  - New Step 15 (after Step 14 GPU Detection from Phase 6): Start DCGM Exporter sidecar if FL_DCGM_ENABLED=YES AND FL_GPU_ENABLED=YES AND GPU detection succeeded. Docker pull + docker run. If pull fails (no network), log warning and continue without DCGM (degraded monitoring, not fatal).
  - After Step 4 (validate_config): configure JSON formatter on flwr logger if FL_LOG_FORMAT=json (same as SuperLink).
- Validation rules for new variables (in validate_config):
  - FL_LOG_FORMAT: must be "text" or "json". Error: "Invalid FL_LOG_FORMAT: '${VALUE}'. Must be 'text' or 'json'."
  - FL_METRICS_ENABLED: must be "YES" or "NO". Standard boolean validation.
  - FL_METRICS_PORT: integer 1024-65535, must not be 9091, 9092, or 9093. Error: "Invalid FL_METRICS_PORT: '${VALUE}'. Must be integer 1024-65535, not 9091-9093."
  - FL_DCGM_ENABLED: must be "YES" or "NO". If YES and FL_GPU_ENABLED != YES: warning "FL_DCGM_ENABLED=YES but FL_GPU_ENABLED is not YES. DCGM will not start."

**Section 11: Failure Modes and Recovery.** Table format consistent with Phases 2, 4, 7:
| Condition | Symptom | Impact | Recovery |
| FL metrics exporter port conflict | "Address already in use" on 9101 | No FL metrics exposed | Change FL_METRICS_PORT to unused port |
| DCGM Exporter SYS_ADMIN denied | Empty /metrics response or crash | No GPU metrics | Verify --cap-add SYS_ADMIN in docker run |
| DCGM image pull fails (no network) | Container not started | No GPU metrics | Ensure network access at boot or pre-pull DCGM image |
| DCGM version mismatch | Missing metrics or incorrect values | Incomplete GPU data | Use DCGM Exporter 4.5.1+ with NVIDIA driver 545+ |
| Prometheus cannot scrape ports | Target "down" in Prometheus | No metrics collected | Open ports 9101/9400 in security groups |
| JSON formatter double-encoding | Broken JSON in log lines | Log parsing fails | Formatter must replace handler, not wrap pre-formatted output |
| Unbounded metric cardinality | Prometheus OOM over time | Monitoring crash | Do not use "round" as label; use single gauge updated per round |

**Section 12: Anti-Patterns.** Table format:
| Anti-Pattern | Why It Fails | Do Instead |
| Running Prometheus/Grafana inside appliance VM | Resource contention with FL training; appliance is immutable | Deploy monitoring on separate operator-managed infrastructure |
| Parsing nvidia-smi for continuous GPU metrics | Fragile text parsing, no timestamps, misses multi-GPU/MIG | Use DCGM Exporter with structured Prometheus metrics |
| Adding prometheus_client to Flower Docker image | Modifies base image, version coupling | Embed in ServerApp FAB dependencies |
| Pushing metrics from container to Prometheus | Violates pull model, adds complexity | Expose /metrics HTTP endpoint; let Prometheus scrape |
| Using "round" as Prometheus label on counters | Unbounded cardinality (1000 rounds = 1000 time series) | Single gauge updated per round; Prometheus stores history |
| Enabling FL_DCGM_ENABLED without FL_GPU_ENABLED | DCGM cannot access GPU management interfaces | Set FL_GPU_ENABLED=YES first (Phase 6 prerequisite) |

**Section 13: New Contextualization Variables Summary.** Table listing all 4 new Phase 8 variables with complete USER_INPUT definitions in copy-paste ready format:
- FL_LOG_FORMAT: O|list|Log output format|text,json|text -- Service-level (both appliances via OneFlow)
- FL_METRICS_ENABLED: O|boolean|Enable Prometheus metrics exporter||NO -- SuperLink
- FL_METRICS_PORT: O|number|Prometheus metrics exporter port||9101 -- SuperLink
- FL_DCGM_ENABLED: O|boolean|Enable DCGM GPU metrics exporter||NO -- SuperNode

End the file with: "Specification for OBS-01, OBS-02: Monitoring and Observability / Phase: 08 - Monitoring and Observability / Version: 1.0"
  </action>
  <verify>
Check that the complete file covers all 4 success criteria:
- grep "Grafana" spec/13-monitoring-observability.md (SC3 dashboards)
- grep "FLTrainingStalled" spec/13-monitoring-observability.md (SC4 alerting)
- grep "GPUMemoryExhaustion" spec/13-monitoring-observability.md (SC4 alerting)
- grep "Anti-Pattern" spec/13-monitoring-observability.md
- grep "Boot Sequence" spec/13-monitoring-observability.md
- wc -l spec/13-monitoring-observability.md (expect 500+ lines)
  </verify>
  <done>spec/13-monitoring-observability.md is complete with all 13 sections covering structured logging (12 events), Prometheus metrics (11 FL + 8 DCGM), 3 Grafana dashboards, 8 alerting rules, boot sequence integration, failure modes, and anti-patterns. All 4 success criteria addressable.</done>
</task>

</tasks>

<verification>
After both tasks complete, verify:
1. spec/13-monitoring-observability.md exists with 500+ lines
2. All 4 success criteria covered:
   - SC1 (structured logging): grep "FL Event Taxonomy" and 12 event types listed
   - SC2 (Prometheus metrics): grep "fl_round_current" and "DCGM_FI_DEV_GPU_UTIL"
   - SC3 (Grafana dashboards): grep "FL Training Overview" and "GPU Health" and "Client Health"
   - SC4 (alerting rules): grep "FLTrainingStalled" and "GPUMemoryExhaustion" and "GPUTemperatureCritical"
3. New CONTEXT variables defined: FL_LOG_FORMAT, FL_METRICS_ENABLED, FL_METRICS_PORT, FL_DCGM_ENABLED
4. Port allocation table present showing 9101 and 9400
5. Requirements OBS-01 and OBS-02 present in header
6. Two-tier structure clear: OBS-01 standalone, OBS-02 builds on it
</verification>

<success_criteria>
- spec/13-monitoring-observability.md is a complete, self-contained spec section
- Structured JSON log format defined with 12 FL event types and specific data fields per event
- 11 Flower training Prometheus metrics defined with types, labels, and descriptions
- DCGM Exporter sidecar defined with 8 GPU metrics and systemd unit
- 3 Grafana dashboards defined with panel types and PromQL queries
- 8 alerting rules (4 FL + 4 GPU) defined with PromQL expressions and severity
- 4 new CONTEXT variables (FL_LOG_FORMAT, FL_METRICS_ENABLED, FL_METRICS_PORT, FL_DCGM_ENABLED) fully specified
- Boot sequence integration documented for both SuperLink and SuperNode
</success_criteria>

<output>
After completion, create `.planning/phases/08-monitoring-and-observability/08-01-SUMMARY.md`
</output>
