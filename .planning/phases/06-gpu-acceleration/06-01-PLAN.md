---
phase: 06-gpu-acceleration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - spec/10-gpu-passthrough.md
autonomous: true

must_haves:
  truths:
    - "A reader can configure a KVM host for NVIDIA GPU passthrough following the spec"
    - "A reader can create a GPU-enabled VM template with UEFI/q35/host-passthrough following the spec"
    - "A reader can install NVIDIA Container Toolkit inside a VM following the spec"
    - "A reader can configure GPU memory management for PyTorch and TensorFlow following the spec"
    - "A reader can implement CPU-only fallback in ClientApp code following the spec"
  artifacts:
    - path: "spec/10-gpu-passthrough.md"
      provides: "Complete GPU passthrough stack specification"
      min_lines: 400
      contains: ["IOMMU", "vfio-pci", "UEFI", "q35", "nvidia-container-toolkit", "set_per_process_memory_fraction", "CPU-only fallback"]
  key_links:
    - from: "spec/10-gpu-passthrough.md"
      to: "spec/02-supernode-appliance.md"
      via: "References SuperNode for GPU-enabled boot sequence additions"
      pattern: "SuperNode"
    - from: "spec/10-gpu-passthrough.md"
      to: "spec/06-ml-framework-variants.md"
      via: "GPU variants extend CPU-only framework images"
      pattern: "flower-supernode-pytorch"
---

<objective>
Create the complete GPU passthrough stack specification for accelerated federated learning on SuperNode appliances

Purpose: Enable GPU acceleration for FL training workloads by specifying the full stack from host IOMMU configuration through container runtime to application-level CUDA memory management. This addresses ML-02 requirement.

Output: spec/10-gpu-passthrough.md covering the 4-layer GPU stack (host, VM template, container runtime, application), with CPU-only fallback path and decision records
</objective>

<execution_context>
@/home/pablo/.claude/get-shit-done/workflows/execute-plan.md
@/home/pablo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-gpu-acceleration/06-RESEARCH.md

# Prior phase artifacts
@spec/02-supernode-appliance.md
@spec/06-ml-framework-variants.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write GPU passthrough stack specification (Layers 1-2)</name>
  <files>spec/10-gpu-passthrough.md</files>
  <action>
Create spec/10-gpu-passthrough.md with the following structure:

1. **Header and Overview**
   - Title: "GPU Passthrough Stack"
   - Purpose: GPU acceleration for FL training workloads on SuperNode appliances
   - 4-layer architecture overview diagram (from research)

2. **Layer 1: Host Prerequisites**
   - BIOS Configuration: Intel VT-d or AMD IOMMU enablement (manual step)
   - Kernel Parameters: `intel_iommu=on iommu=pt` or `amd_iommu=on iommu=pt`
   - vfio-pci module loading (`/etc/modules-load.d/vfio-pci.conf`)
   - GPU driver binding with driverctl: `driverctl set-override <pci_address> vfio-pci`
   - udev rules for VFIO permissions (`/etc/udev/rules.d/99-vfio.rules`)
   - Verification commands for each step

3. **OpenNebula PCI Device Discovery**
   - pci.conf filter configuration (`:filter: '10de:*'` for NVIDIA)
   - `onehost sync -f` and probe timing
   - `onehost show` verification

4. **Layer 2: VM Template Requirements**
   - OpenNebula VM template attributes (exact syntax from research):
     - OS = [ ARCH = "x86_64", FIRMWARE = "UEFI" ]
     - FEATURES = [ MACHINE = "q35" ]
     - CPU_MODEL = [ MODEL = "host-passthrough" ]
     - TOPOLOGY = [ PIN_POLICY = "CORE", CORES = n, SOCKETS = "1" ]
     - PCI = [ SHORT_ADDRESS = "<gpu_pci_address>" ]
   - Why each attribute is required (UEFI for Resizable BAR, q35 for PCIe, etc.)
   - NUMA-aware CPU pinning recommendation

5. **Anti-Patterns (Layers 1-2)**
   - Missing UEFI firmware (modern GPUs fail with SeaBIOS)
   - GPU bound to nouveau/nvidia instead of vfio-pci
   - IOMMU disabled in BIOS despite kernel params
   - Missing NUMA pinning (cross-NUMA scheduling penalty)

Use the exact patterns from 06-RESEARCH.md. Include the 4-layer stack diagram. Match the documentation style of existing spec files (01-superlink-appliance.md as reference).
  </action>
  <verify>
    - spec/10-gpu-passthrough.md exists
    - Contains Layer 1 (Host Prerequisites) section with IOMMU, vfio-pci, driverctl
    - Contains Layer 2 (VM Template Requirements) section with UEFI, q35, host-passthrough, PCI
    - Contains OpenNebula PCI Device Discovery section
  </verify>
  <done>
    Layers 1-2 of GPU passthrough stack documented with exact configuration syntax and verification commands
  </done>
</task>

<task type="auto">
  <name>Task 2: Write GPU passthrough stack specification (Layers 3-4)</name>
  <files>spec/10-gpu-passthrough.md</files>
  <action>
Append to spec/10-gpu-passthrough.md the container runtime and application layers:

6. **Layer 3: Container Runtime (NVIDIA Container Toolkit)**
   - Prerequisites: GPU visible in VM via `lspci | grep NVIDIA`
   - NVIDIA driver installation inside VM (`apt install nvidia-driver-545`)
   - NVIDIA Container Toolkit installation (exact apt commands from research)
   - Docker configuration: `nvidia-ctk runtime configure --runtime=docker`
   - Verification: `docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi`

7. **SuperNode Docker Run Modification**
   - When `FL_GPU_ENABLED=YES`, add `--gpus all` flag
   - CUDA_VISIBLE_DEVICES environment variable for GPU selection
   - Example docker run command (GPU-enabled variant)
   - Comparison with CPU-only docker run from spec/02-supernode-appliance.md

8. **Layer 4: Application CUDA Memory Management**
   - PyTorch memory configuration:
     - `torch.cuda.set_per_process_memory_fraction(fraction)` (soft limit -- document caveat)
     - `CUDA_VISIBLE_DEVICES` environment variable
   - TensorFlow memory configuration:
     - `tf.config.experimental.set_memory_growth(gpu, True)` (dynamic allocation)
     - `tf.config.set_logical_device_configuration()` (hard limit)
     - `TF_FORCE_GPU_ALLOW_GROWTH=true` environment variable
   - When to use each approach (single vs multi-client scenarios)

9. **CPU-Only Fallback Path**
   - Design principle: Every ClientApp must handle no-GPU case
   - PyTorch fallback pattern (from research):
     ```python
     device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
     ```
   - TensorFlow fallback pattern (from research):
     ```python
     gpus = tf.config.list_physical_devices('GPU')
     if not gpus:
         print("No GPU available, using CPU")
     ```
   - Logging requirement: ClientApp MUST log device being used
   - Boot sequence behavior: `FL_GPU_ENABLED=YES` with no GPU is WARNING, not FATAL

10. **Anti-Patterns (Layers 3-4)**
    - Assuming GPU is always present (`.cuda()` without availability check)
    - TensorFlow pre-allocating all GPU memory (missing set_memory_growth)
    - Missing nvidia-ctk runtime configure step
    - PyTorch memory fraction as hard limit (it's a soft limit)

Use code examples from 06-RESEARCH.md. Include the exact Python patterns for memory configuration and fallback.
  </action>
  <verify>
    - spec/10-gpu-passthrough.md contains Layer 3 (Container Runtime) section
    - Contains Layer 4 (Application CUDA) section with PyTorch and TensorFlow patterns
    - Contains CPU-Only Fallback Path section
    - Includes docker run example with --gpus flag
  </verify>
  <done>
    Layers 3-4 of GPU passthrough stack documented with Container Toolkit installation, memory management, and CPU fallback patterns
  </done>
</task>

<task type="auto">
  <name>Task 3: Add decision records and cross-references</name>
  <files>spec/10-gpu-passthrough.md</files>
  <action>
Complete spec/10-gpu-passthrough.md with decision records and cross-references:

11. **Decision Records**
    - DR-01: Full GPU passthrough over vGPU (license-free, near-bare-metal performance)
    - DR-02: driverctl for persistent driver binding over init scripts (survives kernel updates)
    - DR-03: Memory growth over memory fraction as default (simpler, works for single-client)
    - DR-04: Defer MIG to future phase (complex host setup, limited OpenNebula docs)
    - DR-05: CPU fallback is WARNING not FATAL (degraded but functional training)

12. **Contextualization Variables Preview**
    Table of new FL_GPU_* variables introduced by this phase:
    | Variable | Type | Default | Description |
    |----------|------|---------|-------------|
    | FL_GPU_ENABLED | boolean | NO | Enable GPU passthrough to container |
    | FL_CUDA_VISIBLE_DEVICES | string | all | GPU device IDs visible to container |
    | FL_GPU_MEMORY_FRACTION | float | 0.8 | GPU memory fraction (PyTorch only) |

    Note: Full USER_INPUT definitions added to contextualization reference in Plan 06-02.

13. **Cross-References**
    - spec/02-supernode-appliance.md: GPU detection to be added in boot sequence
    - spec/06-ml-framework-variants.md: GPU variants extend CPU-only framework images
    - spec/03-contextualization-reference.md: FL_GPU_* variables added in Plan 06-02

14. **Open Questions**
    - MIG profile support in OpenNebula Sunstone UI (deferred to future phase)
    - NVIDIA driver version pinning strategy (recommendation: nvidia-driver-545 or nvidia-driver-550)

Ensure spec follows the structure of existing specs (01-superlink-appliance.md style). Include section numbers matching the spec file numbering convention.
  </action>
  <verify>
    - spec/10-gpu-passthrough.md contains Decision Records section with DR-01 through DR-05
    - Contains Contextualization Variables Preview table
    - Contains Cross-References section
    - Total file is at least 400 lines
  </verify>
  <done>
    Complete GPU passthrough stack specification with all 4 layers, decision records, and cross-references to existing specs
  </done>
</task>

</tasks>

<verification>
Phase 6 Plan 01 verification:

1. **Layer 1 coverage:** Host IOMMU, vfio-pci, driverctl, udev rules documented
2. **Layer 2 coverage:** VM template with UEFI, q35, host-passthrough, PCI assignment documented
3. **Layer 3 coverage:** NVIDIA Container Toolkit installation and Docker configuration documented
4. **Layer 4 coverage:** PyTorch and TensorFlow memory management documented
5. **CPU fallback:** Fallback patterns for both frameworks documented
6. **Anti-patterns:** Common pitfalls documented for each layer
7. **Decision records:** 5 ADR-style decision records included
8. **Cross-references:** Links to SuperNode spec, framework variants spec, contextualization reference
</verification>

<success_criteria>
- spec/10-gpu-passthrough.md exists with 400+ lines
- All 4 GPU stack layers are fully documented
- CPU-only fallback path is specified
- Decision records explain key architectural choices
- Cross-references to prior phase specs are included
- Spec follows the documentation style of existing spec files
</success_criteria>

<output>
After completion, create `.planning/phases/06-gpu-acceleration/06-01-SUMMARY.md` using the summary template
</output>
