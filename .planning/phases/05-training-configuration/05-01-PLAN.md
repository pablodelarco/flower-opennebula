---
phase: 05-training-configuration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - spec/09-training-configuration.md
autonomous: true

must_haves:
  truths:
    - "A reader can identify all 6 supported aggregation strategies (FedAvg, FedProx, FedAdam, Krum, Bulyan, FedTrimmedAvg) with their Flower class names, exposed parameters, and when to use each"
    - "A reader can trace the complete path from FL_STRATEGY context variable through configure.sh to ServerApp strategy instantiation"
    - "A reader can configure checkpointing via FL_CHECKPOINT_ENABLED, FL_CHECKPOINT_INTERVAL, and FL_CHECKPOINT_PATH using only contextualization variables"
    - "A reader can understand what happens when a SuperLink or SuperNode crashes mid-training and how checkpoints enable resumption"
  artifacts:
    - path: "spec/09-training-configuration.md"
      provides: "Complete training configuration specification covering aggregation strategies and checkpointing"
      min_lines: 400
      contains: "ML-01"
  key_links:
    - from: "FL_STRATEGY context variable"
      to: "ServerApp strategy factory"
      via: "configure.sh -> run_config -> context.run_config"
      pattern: "STRATEGY_MAP"
    - from: "FL_CHECKPOINT_ENABLED"
      to: "evaluate_fn callback"
      via: "run_config -> checkpoint save logic"
      pattern: "checkpoint_round_"
    - from: "Checkpoint file"
      to: "Resume workflow"
      via: "initial_arrays from saved checkpoint"
      pattern: "checkpoint_latest"
---

<objective>
Write the core training configuration specification (spec/09-training-configuration.md) covering aggregation strategy selection (ML-01) and model checkpointing (ML-04).

Purpose: This is the primary deliverable for Phase 5 -- a new spec section that defines how users select aggregation strategies and configure training parameters through contextualization, how the ServerApp instantiates strategies from run_config, how checkpoints are saved and resumed, and how failure recovery works.

Output: `spec/09-training-configuration.md` -- a complete, self-contained spec section following the established format from Phases 1-4.
</objective>

<execution_context>
@/home/pablo/.claude/get-shit-done/workflows/execute-plan.md
@/home/pablo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-training-configuration/05-RESEARCH.md

@spec/00-overview.md
@spec/01-superlink-appliance.md
@spec/03-contextualization-reference.md
@spec/07-use-case-templates.md
@spec/08-single-site-orchestration.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write aggregation strategy specification (Sections 1-5)</name>
  <files>spec/09-training-configuration.md</files>
  <action>
Create `spec/09-training-configuration.md` with the following structure. Use the same format and conventions as existing spec sections (see spec/01-superlink-appliance.md and spec/08-single-site-orchestration.md for style reference).

**Header:**
```
# Training Configuration
**Requirements:** ML-01, ML-04
**Phase:** 05 - Training Configuration
**Status:** Specification
```

**Section 1: Purpose and Scope**
- What the section covers: aggregation strategy selection, strategy-specific parameters, checkpointing mechanism, failure recovery
- What it does NOT cover: monitoring (Phase 8), auto-scaling (Phase 9), GPU-specific training config (Phase 6)
- Cross-references to: spec/01-superlink-appliance.md (boot sequence, SuperLink params), spec/03-contextualization-reference.md (variable definitions), spec/07-use-case-templates.md (pre-built FABs), spec/08-single-site-orchestration.md (service template)

**Section 2: Aggregation Strategy Reference**
For EACH of the 6 strategies (FedAvg, FedProx, FedAdam, Krum, Bulyan, FedTrimmedAvg), include:
- Algorithm summary (1-2 sentences explaining what it does)
- When to use (data distribution, client heterogeneity, security concerns)
- Flower class: `flwr.server.strategy.{ClassName}`
- Parameter table: Parameter | Type | Default | Contextualization Variable | Notes
- Minimum client requirements (formula for Krum: n >= 2f+3, Bulyan: n >= 4f+3)
- Important caveats (e.g., FedProx requires client-side proximal term, FedAdam requires initial_parameters)

Use the detailed strategy information from 05-RESEARCH.md Aggregation Strategy Reference section.

**Section 3: Strategy Selection Architecture**
Document the complete path from contextualization variable to running strategy:

1. FL_STRATEGY context variable (set in VM template or OneFlow service user_inputs)
2. configure.sh reads FL_STRATEGY and strategy-specific parameters (FL_PROXIMAL_MU, FL_SERVER_LR, etc.)
3. configure.sh generates run_config key-value pairs (strategy=FedProx, proximal-mu=0.1, etc.)
4. run_config passed to FAB via --run-config flag or written to pyproject.toml [tool.flwr.app.config]
5. ServerApp reads context.run_config and instantiates the correct strategy class

Include the configure.sh `generate_run_config()` bash function from the research (with the case statement for strategy-specific parameters).

Include the ServerApp strategy factory Python pattern (STRATEGY_MAP with _build_* functions and _common_params helper) from the research.

**Anti-patterns subsection:** Include the 4 anti-patterns from research:
- Hand-rolling strategy selection outside the FAB
- Assuming automatic checkpointing exists
- Mounting checkpoints on SuperNode
- Using in-container paths for FL_CHECKPOINT_PATH

**Section 4: Strategy-Specific Parameter Variables (New Phase 5 Variables)**
Table of all 8 new contextualization variables with complete definitions:

| Variable | USER_INPUT Definition | Type | Default | Validation Rule | Flower Mapping | Applies When |
(Use the exact variable definitions from the research: FL_PROXIMAL_MU, FL_SERVER_LR, FL_CLIENT_LR, FL_NUM_MALICIOUS, FL_TRIM_BETA, FL_CHECKPOINT_ENABLED, FL_CHECKPOINT_INTERVAL, FL_CHECKPOINT_PATH)

Also document the FL_STRATEGY update (extending from 3 to 6 options: add Krum, Bulyan, FedTrimmedAvg).

Include a USER_INPUT block (copy-paste ready) for the new SuperLink variables.

**Section 5: Strategy Validation Rules**
Boot-time validation in configure.sh for Phase 5 variables:
- FL_PROXIMAL_MU: non-negative float (>=0.0). Warn if FL_STRATEGY != FedProx.
- FL_SERVER_LR / FL_CLIENT_LR: positive float (>0.0). Warn if FL_STRATEGY != FedAdam.
- FL_NUM_MALICIOUS: non-negative integer. If FL_STRATEGY=Krum, validate n >= 2f+3. If Bulyan, validate n >= 4f+3. Use FL_MIN_AVAILABLE_CLIENTS for n.
- FL_TRIM_BETA: float in range 0.0-0.5 exclusive. Warn if FL_STRATEGY != FedTrimmedAvg.
- FL_CHECKPOINT_INTERVAL: positive integer (>0). Ignored if FL_CHECKPOINT_ENABLED != YES.
- FL_CHECKPOINT_PATH: non-empty string.
- Conditional validation: strategy-specific params are validated only when FL_STRATEGY matches. Log "INFO" when ignoring irrelevant params.

Include validation pseudocode in bash following the existing pattern from spec/03-contextualization-reference.md Section 8.

Include the FedProx client-side warning: if FL_STRATEGY=FedProx, log a notice that pre-built FABs support the proximal term but custom ClientApps must implement it manually.
  </action>
  <verify>
Verify spec/09-training-configuration.md exists and contains Sections 1-5.
Grep for all 6 strategy names: FedAvg, FedProx, FedAdam, Krum, Bulyan, FedTrimmedAvg.
Grep for all 8 new variable names: FL_PROXIMAL_MU, FL_SERVER_LR, FL_CLIENT_LR, FL_NUM_MALICIOUS, FL_TRIM_BETA, FL_CHECKPOINT_ENABLED, FL_CHECKPOINT_INTERVAL, FL_CHECKPOINT_PATH.
Grep for STRATEGY_MAP (ServerApp factory pattern).
Grep for generate_run_config (configure.sh bridge).
  </verify>
  <done>
Sections 1-5 of spec/09-training-configuration.md are complete with all 6 strategies documented, all 8 new variables defined, the configure.sh bridge function, the ServerApp factory pattern, and validation pseudocode.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write checkpointing specification and failure recovery (Sections 6-9)</name>
  <files>spec/09-training-configuration.md</files>
  <action>
Append Sections 6-9 to the existing spec/09-training-configuration.md:

**Section 6: Model Checkpointing**
Cover the complete checkpointing mechanism:

6a. Checkpointing is NOT automatic in Flower -- requires explicit implementation in ServerApp evaluate_fn callback.

6b. Checkpoint file format: .npz (NumPy) as default (framework-agnostic via ArrayRecord). Include the table from research showing PyTorch (.pt), TensorFlow (.keras), scikit-learn (.npz) options. Recommend .npz as default because ArrayRecord always converts to NumPy.

6c. Checkpoint naming convention:
```
/app/checkpoints/
    checkpoint_round_{N}.npz
    checkpoint_latest.npz (symlink to most recent)
    checkpoint_latest.json (metadata: round, timestamp, num_arrays)
```

6d. Checkpoint evaluate_fn implementation -- include the `make_checkpoint_fn` Python code from research with:
- Round-based saving (every N rounds where N = FL_CHECKPOINT_INTERVAL)
- Always save on final round
- checkpoint_latest.json metadata
- checkpoint_latest.npz symlink management

6e. Checkpoint volume mount:
- Host path: /opt/flower/checkpoints (created by configure.sh, chown 49999:49999)
- Docker mount: -v /opt/flower/checkpoints:/app/checkpoints:rw
- Only added when FL_CHECKPOINT_ENABLED=YES
- Follows same pattern as /opt/flower/state:/app/state

6f. configure.sh additions for checkpointing:
- mkdir -p /opt/flower/checkpoints
- chown 49999:49999 /opt/flower/checkpoints
- Docker run command extension with -v mount
- Conditional: only when FL_CHECKPOINT_ENABLED=YES

**Section 7: Resume from Checkpoint**
Document the resume workflow:
- Flower has no built-in resume mechanism
- Resume is implemented by loading saved checkpoint as initial_arrays
- ServerApp checks FL_CHECKPOINT_PATH for checkpoint_latest.npz at startup
- If found: load as initial_arrays, log "Resuming from checkpoint"
- If not found: initialize fresh model
- Include the Python resume pattern from research
- Important: round counter restarts from 1. If user wants to train for remaining rounds, set FL_NUM_ROUNDS to remaining count.
- Document FL_RESUME_ROUND as NOT implemented (Flower has no round offset concept). The operator must adjust FL_NUM_ROUNDS manually.

**Section 8: Storage Backend Options**
Document the 4 storage options from research:
| Backend | How | Pros | Cons |
(Local disk, Persistent volume/OpenNebula DISK, NFS mount, S3 upload)

Recommendation hierarchy:
1. Default: Local disk at /opt/flower/checkpoints on root disk. Simple, fast, survives container restarts but NOT VM termination.
2. Persistent: Attach secondary DISK in VM template, mount at /opt/flower/checkpoints. Survives VM termination.
3. NFS/S3: Document as operator-managed. Appliance writes to local path; what backs that path is the operator's choice.

The appliance does NOT manage disk attachment or NFS/S3 -- that is infrastructure concern.

**Section 9: Failure Recovery**
Document 4 failure scenarios from research:

Scenario 1: SuperNode crashes mid-training round
- Strategy's accept_failures=True handles gracefully
- Round continues with remaining clients
- SuperNode reconnects automatically
- No checkpoint involvement (SuperNode has no persistent state)

Scenario 2: SuperLink crashes mid-training round
- Systemd restarts container
- state.db persists run history
- WITHOUT checkpoints: model weights lost, training restarts from scratch
- WITH checkpoints: ServerApp loads checkpoint_latest.npz as initial_arrays
- SuperNodes reconnect and receive restored model
- Checkpoint is critical for SuperLink crash recovery

Scenario 3: Full service redeployment (VM terminated)
- Local disk checkpoints lost unless on persistent volume
- With persistent volume: new VM attaches same volume, resumes
- Without: training starts fresh

Scenario 4: Network partition (temporary)
- SuperNodes reconnect when network recovers
- No checkpoint involvement
- Flower native reconnection handles this

Include a summary table:
| Scenario | Checkpoint Role | Data Loss Without Checkpoint | Recovery Time |

**Spec footer:**
```
*Specification for ML-01 and ML-04: Training Configuration*
*Phase: 05 - Training Configuration*
*Version: 1.0*
```
  </action>
  <verify>
Verify spec/09-training-configuration.md contains Sections 6-9.
Grep for "checkpoint_round_" (naming convention).
Grep for "evaluate_fn" or "make_checkpoint_fn" (checkpoint callback).
Grep for "Failure Recovery" (Section 9 header).
Grep for "SuperNode crashes" and "SuperLink crashes" (failure scenarios).
Grep for "checkpoint_latest" (symlink pattern).
Grep for "Version: 1.0" (spec footer present).
Count total lines -- should be 400+.
  </verify>
  <done>
spec/09-training-configuration.md is complete with all 9 sections: purpose/scope, strategy reference (6 strategies), selection architecture (configure.sh bridge + ServerApp factory), parameter variables (8 new vars), validation rules, checkpointing mechanism, resume workflow, storage backends, and failure recovery (4 scenarios). Spec footer present.
  </done>
</task>

</tasks>

<verification>
- `spec/09-training-configuration.md` exists and is 400+ lines
- All 6 strategies documented with parameters, when-to-use, and Flower class
- All 8 new contextualization variables defined with USER_INPUT format
- FL_STRATEGY update documented (3 -> 6 options)
- configure.sh bridge function (generate_run_config) present
- ServerApp strategy factory pattern (STRATEGY_MAP) present
- Checkpointing mechanism fully specified (evaluate_fn, naming, volume mount)
- Resume workflow documented
- 4 failure recovery scenarios documented
- Validation pseudocode with byzantine client count checks present
</verification>

<success_criteria>
A reader can:
1. Select any of the 6 supported aggregation strategies and know which parameters to set
2. Trace the complete path from FL_STRATEGY to running strategy in the ServerApp
3. Enable checkpointing by setting FL_CHECKPOINT_ENABLED=YES and understand the file format, naming, and storage options
4. Understand what happens during each failure scenario and how checkpoints enable recovery
</success_criteria>

<output>
After completion, create `.planning/phases/05-training-configuration/05-01-SUMMARY.md`
</output>
