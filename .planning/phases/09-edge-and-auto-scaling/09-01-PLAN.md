---
phase: 09-edge-and-auto-scaling
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - spec/14-edge-and-auto-scaling.md
autonomous: true

must_haves:
  truths:
    - "A reader can identify every difference between the edge SuperNode and the standard SuperNode (base OS, Docker image, resource footprint, target QCOW2 size)"
    - "A reader can understand how an edge SuperNode handles intermittent connectivity -- retry, backoff, partial participation semantics"
    - "A reader can define OneFlow elasticity policies for the SuperNode role with expression-based triggers and cooldown periods"
    - "A reader can understand what happens to an active training round when a SuperNode is scaled in or an edge node disconnects"
    - "A reader can configure auto-scaling with FL-aware custom metrics published via OneGate"
  artifacts:
    - path: "spec/14-edge-and-auto-scaling.md"
      provides: "Complete edge and auto-scaling specification"
      min_lines: 500
      contains: "EDGE-01"
  key_links:
    - from: "spec/14-edge-and-auto-scaling.md"
      to: "spec/02-supernode-appliance.md"
      via: "edge variant differences table"
      pattern: "spec/02-supernode-appliance"
    - from: "spec/14-edge-and-auto-scaling.md"
      to: "spec/08-single-site-orchestration.md"
      via: "elasticity_policies extending SuperNode role"
      pattern: "spec/08-single-site-orchestration"
    - from: "spec/14-edge-and-auto-scaling.md"
      to: "spec/12-multi-site-federation.md"
      via: "edge deployment as remote training site variant"
      pattern: "spec/12-multi-site-federation"
---

<objective>
Create the complete edge and auto-scaling specification document (spec/14-edge-and-auto-scaling.md) covering both requirements: EDGE-01 (edge-optimized SuperNode appliance) and ORCH-03 (OneFlow auto-scaling via elasticity rules).

Purpose: This is the final phase of the specification project. It defines how the Flower-OpenNebula integration supports constrained edge environments (bandwidth-limited, intermittent connectivity) and dynamic client scaling during training (auto-scaling SuperNode cardinality based on load and FL-aware metrics).

Output: `spec/14-edge-and-auto-scaling.md` -- a self-contained specification document following the established structure from prior phases.
</objective>

<execution_context>
@/home/pablo/.claude/get-shit-done/workflows/execute-plan.md
@/home/pablo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-edge-and-auto-scaling/09-RESEARCH.md

# Existing specs this builds on
@spec/02-supernode-appliance.md
@spec/08-single-site-orchestration.md
@spec/12-multi-site-federation.md
@spec/03-contextualization-reference.md
@spec/00-overview.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write edge SuperNode appliance specification (EDGE-01)</name>
  <files>spec/14-edge-and-auto-scaling.md</files>
  <action>
Create `spec/14-edge-and-auto-scaling.md` with the following sections covering EDGE-01:

**Document header:**
- Requirement: ORCH-03, EDGE-01
- Phase: 09 - Edge and Auto-Scaling
- Status: Specification

**Section 1: Purpose and Scope**
- What this section covers: edge SuperNode appliance variant, intermittent connectivity handling, OneFlow elasticity policies, client join/leave semantics
- What this section does NOT cover: standard SuperNode (Phase 1), multi-site networking (Phase 7), monitoring (Phase 8)
- Cross-references to: spec/02 (standard SuperNode), spec/08 (orchestration), spec/12 (multi-site), spec/03 (contextualization), spec/13 (monitoring)

**Section 2: Edge SuperNode Appliance Variant**
- Differences table: standard vs edge SuperNode (base OS, Docker image, framework, QCOW2 size, resources, connectivity model, retry config)
- Base OS: Ubuntu 24.04 Minimal Cloud Image (~300-400MB) instead of standard Ubuntu 24.04 (~800MB)
- Docker image: base `flwr/supernode:1.25.0` only (~190MB), NO framework pre-baked
- Target QCOW2 size: <2GB (estimated 1.0-1.5GB)
- Recommended VM resources: 2 vCPU, 2-4GB RAM, 10-20GB disk
- Image components table (mirroring spec/02 Section 2 format but stripped down)
- Build optimization: Ubuntu Minimal + Docker CE + base Flower only + jq/curl/nc + `docker system prune` + `apt-get clean`
- Size breakdown table (component-by-component estimate)
- Pre-baked image strategy: same as standard (network-free boot even more critical at edge)
- Framework provisioning: user provides custom Docker image via `FL_ISOLATION=process` or pulls at runtime if network available
- Note Ubuntu Minimal + one-apps contextualization compatibility as implementation validation item (Open Question from research)
- Marketplace identifier: separate QCOW2 "Flower SuperNode - Edge" alongside framework variants

**Section 3: Intermittent Connectivity Handling**
- Three-layer resilience model (build on what research documented):
  1. Flower-native reconnection (`--max-retries 0`, `--max-wait-time 0`) -- already specified Phase 1
  2. OneGate discovery retry (30 retries, 10s interval) -- already specified Phase 1
  3. Edge-specific enhancements (NEW): configurable backoff, bounded retry windows
- Edge backoff configuration: define FL_EDGE_BACKOFF (exponential|fixed, default: exponential) and FL_EDGE_MAX_BACKOFF (max backoff seconds, default: 300)
- Backoff behavior specification: exponential starts at 10s, doubles each attempt, caps at FL_EDGE_MAX_BACKOFF. Fixed uses the existing 10s interval.
- IMPORTANT: Note that Flower's `--max-retries` and `--max-wait-time` control Flower-level reconnection (gRPC layer). The edge backoff applies to the DISCOVERY retry loop only (OneGate layer). Once connected, Flower's native reconnection handles all runtime disconnects.
- Client disconnect mid-round behavior table (from research: 6 scenarios with Flower server behavior and training impact)
- Partial participation tolerance: document `accept_failures=True` (FedAvg default), `FaultTolerantFedAvg` with `min_completion_rate_fit=0.5` recommendation for edge
- Reconnection between rounds: transparent to training (Flower resamples connected clients)
- `min_available_clients` interaction: setting below expected node count prevents single-node failures from blocking training

**Section 4: Edge Deployment Considerations**
- Data provisioning at edge: pre-loaded in QCOW2 or provisioned via `START_SCRIPT`; no reliance on runtime downloads
- Partition-ID stability warning (from research Pitfall 2): recommend explicit `FL_NODE_CONFIG` for edge with expected churn
- Edge VM template (conceptual): 2 vCPU, 2GB RAM, 10GB disk, reduced resource profile
- Network requirements: outbound TCP to SuperLink port 9092 only (same as standard, but emphasize firewalling)
- Bandwidth considerations: model update sizes, gRPC compression, impact on metered connections

**Section 5: Decision Records**
- DR-01: Ubuntu Minimal over Alpine (one-apps compatibility, consistency with standard stack)
- DR-02: Base Flower image only, no framework pre-baked (size target, framework flexibility)
- DR-03: Exponential backoff as default over fixed (better for intermittent WAN)
- DR-04: FaultTolerantFedAvg recommended for edge deployments (tolerates 50% dropout)
</action>
  <verify>
- `spec/14-edge-and-auto-scaling.md` exists
- File contains "EDGE-01" in header
- File has differences table (standard vs edge)
- File has "<2GB" or "under 2 GB" size target
- File has intermittent connectivity section with 3-layer model
- File has FL_EDGE_BACKOFF and FL_EDGE_MAX_BACKOFF variable definitions
- File has client disconnect behavior table (6+ rows)
- File has at least 4 decision records
- File cross-references spec/02, spec/08, spec/12
  </verify>
  <done>EDGE-01 sections of spec/14 are complete with edge SuperNode appliance variant, intermittent connectivity handling, deployment considerations, and decision records</done>
</task>

<task type="auto">
  <name>Task 2: Write OneFlow auto-scaling specification (ORCH-03)</name>
  <files>spec/14-edge-and-auto-scaling.md</files>
  <action>
Append the following sections to `spec/14-edge-and-auto-scaling.md` covering ORCH-03:

**Section 6: OneFlow Auto-Scaling Architecture**
- Overview: elasticity_policies on SuperNode role only (SuperLink singleton, no elasticity -- restate from spec/08 Section 4)
- How OneFlow evaluates expressions: periodic evaluation, average across all running VMs in role, attribute lookup order (USER_TEMPLATE > MONITORING > TEMPLATE > VM)
- Adjustment types: CHANGE (+/- N), CARDINALITY (set to N), PERCENTAGE_CHANGE (+/- N%)
- Period and cooldown: `period` (evaluation interval in seconds), `period_number` (consecutive true evaluations before trigger), `cooldown` (post-scaling lockout in seconds)

**Section 7: Custom FL Metrics via OneGate**
- OneGate PUT API for publishing custom metrics from SuperNode VMs
- Define 3 custom metrics:
  - FL_TRAINING_ACTIVE (YES/NO): published at training round start/end
  - FL_CLIENT_STATUS (IDLE/TRAINING/DISCONNECTED): published on status changes
  - FL_ROUND_NUMBER (integer): published each round
- Publication mechanism: periodic cron job or hook in bootstrap.sh (specify the pattern, not implementation)
- How these appear in USER_TEMPLATE for elasticity expression evaluation
- Note: these are OneGate-published runtime metrics, NOT CONTEXT variables (important distinction)

**Section 8: Elasticity Policy Definitions**
- Complete SuperNode role JSON with elasticity_policies (from research, refined):
  - Scale-up policy: `expression: "CPU > 80"`, period_number: 3, period: 60, cooldown: 300, adjust: +1
  - Scale-down policy: `expression: "CPU < 20"`, period_number: 5, period: 120, cooldown: 600, adjust: -1
- Alternative FL-aware policies using custom metrics (examples):
  - Scale-up when all clients are TRAINING: `expression: "FL_CLIENT_STATUS == TRAINING"` (document that string equality in expressions may need numeric encoding -- recommend 1=IDLE, 2=TRAINING, 3=DISCONNECTED)
- Scheduled policies: cron-based cardinality for predictable workload patterns (business hours example from research)
- Interaction with min_vms and max_vms bounds
- Constraint: `min_vms >= FL_MIN_FIT_CLIENTS` to prevent training deadlock

**Section 9: Client Join/Leave Semantics During Auto-Scaling**
- Scale-up during active training: new SuperNode boots, discovers SuperLink, joins for NEXT round (current round unaffected)
- Scale-down during active training: OneFlow sends shutdown, container SIGTERM, client disconnects mid-round
  - If `accept_failures=True` (default) and enough results: round proceeds
  - If below min_fit_clients: round fails, next round retries with remaining clients
- Newest-VMs-removed-first: document this as OneFlow default behavior, note FL-aware selection not supported
- Cooldown as scale-down protection: 600s cooldown >= typical round duration (recommend longer for long-running rounds)
- Interaction with partition-id: scaled VMs auto-compute partition-id from updated nodes array (existing behavior from spec/08 Section 5)

**Section 10: Anti-Patterns**
- Build on research anti-patterns, formatted as table (matching spec/08 Section 10 format):
  1. Setting min_available_clients equal to SuperNode cardinality at edge
  2. Aggressive scale-down cooldown (< round duration)
  3. Elasticity policies on SuperLink role
  4. Scaling below min_fit_clients (min_vms must be >= FL_MIN_FIT_CLIENTS)
  5. Using framework-specific images for edge (defeats <2GB target)
  6. Averaging masks individual VM issues (recommend custom FL metrics over raw CPU)
  7. Cooldown blocking emergency scale-up

**Section 11: New Contextualization Variables Summary**
- List new variables introduced in Phase 9:
  - FL_EDGE_BACKOFF: `O|list|Edge discovery retry backoff strategy|exponential,fixed|exponential` (SuperNode, edge variant only)
  - FL_EDGE_MAX_BACKOFF: `O|number|Maximum backoff interval in seconds for edge discovery||300` (SuperNode, edge variant only)
- Note: No new SuperLink variables. Auto-scaling configuration is in the OneFlow service template (elasticity_policies JSON), not CONTEXT variables.
- Variable count update: 46 -> 48 total
- USER_INPUT block (copy-paste ready) for edge SuperNode

**Document footer:**
- Specification for ORCH-03, EDGE-01
- Phase: 09 - Edge and Auto-Scaling
- Version: 1.0
</action>
  <verify>
- `spec/14-edge-and-auto-scaling.md` contains "ORCH-03" in header
- File has elasticity_policies JSON block
- File has OneGate PUT examples for custom metrics
- File has client join/leave semantics section
- File has anti-patterns table (7+ entries)
- File has FL_EDGE_BACKOFF and FL_EDGE_MAX_BACKOFF USER_INPUT definitions
- File has variable count note (46 -> 48)
- File ends with version 1.0
- Total file length > 500 lines
  </verify>
  <done>ORCH-03 sections of spec/14 are complete with auto-scaling architecture, custom FL metrics, elasticity policies, client join/leave semantics, anti-patterns, and new contextualization variables</done>
</task>

</tasks>

<verification>
After both tasks complete:

1. `spec/14-edge-and-auto-scaling.md` exists with >500 lines
2. EDGE-01 covered: edge SuperNode variant, <2GB target, intermittent connectivity, backoff config
3. ORCH-03 covered: elasticity policies, custom metrics, join/leave semantics, cooldown
4. Cross-references to spec/02, spec/08, spec/12, spec/03
5. 2 new CONTEXT variables defined (FL_EDGE_BACKOFF, FL_EDGE_MAX_BACKOFF)
6. 7+ anti-patterns documented
7. 4+ decision records
8. All 4 roadmap success criteria addressable from this document
</verification>

<success_criteria>
1. A reader can build an edge SuperNode QCOW2 image from this spec without referencing external docs (SC1)
2. A reader can configure OneFlow elasticity policies for FL-aware auto-scaling (SC3)
3. A reader understands exactly what happens during edge disconnect and auto-scale events (SC2, SC4)
4. All decision records explain WHY, not just WHAT
5. The spec integrates cleanly with the existing 13-document specification set
</success_criteria>

<output>
After completion, create `.planning/phases/09-edge-and-auto-scaling/09-01-SUMMARY.md`
</output>
