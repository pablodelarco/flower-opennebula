---
phase: 01-base-appliance-architecture
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - spec/02-supernode-appliance.md
autonomous: true

must_haves:
  truths:
    - "A reader can identify every component in the SuperNode appliance (base OS, Docker engine, ML framework dependencies, Flower container)"
    - "A reader can trace the SuperNode boot sequence from OS boot through SuperLink discovery to Flower container startup"
    - "The spec defines the dual discovery model: OneGate dynamic discovery (default) and static IP override"
    - "The spec defines the OneGate discovery protocol with retry loop, backoff, and timeout behavior"
    - "The spec defines how the SuperNode connects to and maintains connection with the SuperLink"
  artifacts:
    - path: "spec/02-supernode-appliance.md"
      provides: "Complete SuperNode appliance specification"
      contains: "## SuperNode Appliance"
  key_links:
    - from: "spec/02-supernode-appliance.md"
      to: "OneGate service discovery"
      via: "GET /service to find FL_ENDPOINT from SuperLink role"
      pattern: "FL_ENDPOINT"
    - from: "spec/02-supernode-appliance.md"
      to: "Flower SuperLink"
      via: "--superlink flag pointing to discovered Fleet API address"
      pattern: "--superlink.*:9092"
---

<objective>
Write the SuperNode (Flower client) appliance specification section.

Purpose: This is the second foundational spec section that defines how FL client nodes run as marketplace appliances. The SuperNode is more complex than the SuperLink in one key way: it must discover and connect to the SuperLink before it can function. This section covers the QCOW2 image contents, dual discovery model (OneGate + static), boot sequence with discovery phase, and connection lifecycle. An engineer reading this section should be able to build the SuperNode appliance image and understand how it finds and connects to the SuperLink.

Output: `spec/02-supernode-appliance.md` -- a complete spec section covering APPL-02 (SuperNode appliance with server connectivity, local data mount points, and ML framework selection).
</objective>

<execution_context>
@/home/pablo/.claude/get-shit-done/workflows/execute-plan.md
@/home/pablo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/home/pablo/flower-opennebula/.planning/PROJECT.md
@/home/pablo/flower-opennebula/.planning/ROADMAP.md
@/home/pablo/flower-opennebula/.planning/STATE.md
@/home/pablo/flower-opennebula/.planning/phases/01-base-appliance-architecture/01-CONTEXT.md
@/home/pablo/flower-opennebula/.planning/phases/01-base-appliance-architecture/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write SuperNode appliance spec section</name>
  <files>spec/02-supernode-appliance.md</files>
  <action>
Write `spec/02-supernode-appliance.md` as a complete technical specification section. This is a SPEC DOCUMENT, not code.

The spec section MUST cover these areas in order:

**1. Appliance Overview**
- Role: Flower SuperNode (FL client / local trainer)
- Marketplace appliance type: QCOW2 VM image
- Architecture: Docker-in-VM (single container per VM)
- Key difference from SuperLink: must discover and connect to SuperLink before becoming functional

**2. Image Components**
Specify every component baked into the QCOW2 image:
- Base OS: Ubuntu 24.04 LTS (Noble Numbat) -- same base as SuperLink for consistency
- Docker CE 24+ (minimum version constraint)
- Pre-pulled Docker image: `flwr/supernode:1.25.0` (default tag `1.25.0-py3.13-ubuntu24.04`)
- OpenNebula one-apps contextualization packages (latest)
- Supporting tools: jq, curl (for OneGate interactions)
- Custom scripts at `/opt/flower/scripts/` (configure.sh, bootstrap.sh, health-check.sh, common.sh)
- Note: ML framework libraries are INSIDE the Flower Docker image, not in the VM. Phase 3 defines framework-specific image variants.

**3. File Layout Inside the VM**
```
/etc/one-appliance/service
/opt/flower/scripts/{configure.sh, bootstrap.sh, health-check.sh, common.sh}
/opt/flower/config/supernode.env
/opt/flower/certs/ (placeholder for Phase 2)
/opt/flower/data/ (local data mount point)
/var/log/one-appliance/{flower-configure.log, flower-bootstrap.log}
```

**4. Pre-baked Image Strategy**
- Same strategy as SuperLink: QCOW2 ships with `flwr/supernode:1.25.0` pre-pulled
- `FLOWER_VERSION` override triggers pull with fallback to pre-baked
- IMPORTANT: SuperNode version MUST match SuperLink version. Version mismatch causes gRPC protocol errors.
- Estimated image size: ~2-3 GB (similar to SuperLink)

**5. Recommended VM Resources**
- Minimum: 2 vCPU, 4096 MB RAM, 20 GB disk
- Recommended: 4 vCPU, 8192 MB RAM, 40 GB disk
- Note: training workloads are compute-intensive; real sizing depends on model and dataset
- GPU resources: Phase 6 scope (not applicable in Phase 1)

**6. SuperLink Discovery Model**
This is the most critical section of the SuperNode spec. Detail the dual discovery model:

**6a. Decision Logic (in configure.sh):**
Priority order:
1. If `FL_SUPERLINK_ADDRESS` is set (non-empty) -> use it directly (static mode)
2. Else if OneGate token exists (`/run/one-context/token.txt`) -> query OneGate for SuperLink endpoint (dynamic mode)
3. Else -> fail with clear error: "No SuperLink address provided and OneGate not available"

**6b. Static Discovery Mode:**
- User provides `FL_SUPERLINK_ADDRESS` context variable (e.g., `192.168.1.100:9092`)
- Used for: manual deployments, cross-site federation (Phase 7), testing
- No OneGate dependency
- Simplest path; suitable for standalone/non-OneFlow deployments

**6c. Dynamic Discovery Mode (OneGate):**
- Default for OneFlow service deployments
- SuperNode queries `GET ${ONEGATE_ENDPOINT}/service` with authentication headers
- Parses response JSON to find the "superlink" role's VM and extract `FL_ENDPOINT` from its USER_TEMPLATE
- jq path: `.SERVICE.roles[] | select(.name == "superlink") | .nodes[0].vm_info.VM.USER_TEMPLATE.FL_ENDPOINT`
- NOTE: The exact JSON path needs validation during implementation (see Open Questions in research)

**6d. Discovery Retry Loop:**
Specify retry behavior for OneGate discovery:
- Max retries: 30 (configurable future consideration)
- Retry interval: 10 seconds
- Total maximum wait: 5 minutes (30 * 10s)
- On each retry: query OneGate, check for FL_ENDPOINT, log attempt number
- On success: extract endpoint, log "Discovered SuperLink at <endpoint>"
- On exhaustion: fail with error "SuperLink not found after 30 attempts. Ensure SuperLink VM is deployed and healthy."

**6e. OneGate Connectivity Pre-check:**
Before entering the discovery loop:
- Test OneGate reachability: `curl -s -o /dev/null -w "%{http_code}" ${ONEGATE_ENDPOINT}/vm`
- If OneGate unreachable: log warning "OneGate unreachable at ${ONEGATE_ENDPOINT}; dynamic discovery disabled"
- If `FL_SUPERLINK_ADDRESS` is also empty: fail with error
- If `FL_SUPERLINK_ADDRESS` has a value: proceed with static mode (OneGate failure is non-fatal when static address exists)

**7. Linear Boot Sequence**
Specify the exact boot sequence:
1. OS Boot + one-apps contextualization packages run
2. [configure.sh] Source `/run/one-context/one_env` to read all CONTEXT variables
3. [configure.sh] Validate required variables, set defaults for optional ones
4. [configure.sh] Determine SuperLink address (decision logic from section 6)
   - Static mode: use FL_SUPERLINK_ADDRESS directly
   - Dynamic mode: run OneGate discovery loop with retries
5. [configure.sh] Generate Docker run configuration: write `supernode.env`, prepare volume mounts
6. [configure.sh] Create mount-target directories with UID 49999 ownership (`/opt/flower/data`, `/opt/flower/certs`)
7. [configure.sh] Write systemd unit file for Flower SuperNode container
8. [bootstrap.sh] Wait for Docker daemon readiness
9. [bootstrap.sh] Handle version override (same pattern as SuperLink)
10. [bootstrap.sh] Start Flower SuperNode container via systemd unit
11. [bootstrap.sh] Wait for SuperNode to establish connection to SuperLink (health check)
12. [bootstrap.sh] Publish status to OneGate: FL_NODE_READY=YES, FL_ROLE=supernode
13. [REPORT_READY] Contextualization reports VM as READY

For each step, explain WHAT, WHY, and FAILURE behavior.

**8. Docker Container Configuration**
Specify the Docker run parameters for subprocess mode, no TLS (Phase 1):
- Image: `flwr/supernode:${FLOWER_VERSION:-1.25.0}`
- No port mappings needed (SuperNode connects outbound to SuperLink; no inbound connections)
- Volumes: `/opt/flower/data:/app/data:ro` (local training data, read-only to container)
- Restart policy: `unless-stopped`
- Environment: `FLWR_LOG_LEVEL`
- CLI flags: `--insecure`, `--superlink ${SUPERLINK_ADDRESS}:9092`, `--isolation subprocess`, `--node-config`, `--max-retries 0` (unlimited), `--max-wait-time 0` (unlimited)
- Include the exact Docker run command as reference

**9. Systemd Integration**
- Unit name: `flower-supernode.service`
- After/Requires: `docker.service`
- Same pattern as SuperLink
- Note: SuperNode may start before SuperLink is ready; Flower's built-in `--max-retries` handles ongoing reconnection after initial discovery

**10. Health Check and Readiness**
- Health check for SuperNode: verify container is running (`docker inspect --format='{{.State.Running}}' flower-supernode`)
- Note: unlike SuperLink, the SuperNode does not have an inbound port to check. Container running = healthy.
- READY_SCRIPT approach: check Docker container status
- REPORT_READY=YES: reports after container starts successfully
- Important distinction: SuperNode READY means "container started and attempting connection," not "training in progress"

**11. Connection Lifecycle**
After initial discovery, the connection to SuperLink is Flower's responsibility:
- `--max-retries 0`: unlimited reconnection attempts (Flower built-in)
- `--max-wait-time 0`: no timeout on reconnection
- If SuperLink temporarily goes down, SuperNode will automatically reconnect when it comes back
- The boot scripts handle INITIAL discovery only; ongoing connectivity is Flower-native
- This is a deliberate design choice: do NOT hand-roll reconnection logic

**12. Local Data Mount Point**
- `/opt/flower/data/` directory in the VM, mounted to `/app/data` in the container
- Read-only mount to prevent training from modifying source data
- Users can attach additional storage (Longhorn PV, NFS) to this path via OpenNebula VM template disk/NIC configuration
- The `FL_NODE_CONFIG` parameter passes data-related configuration (partition-id, num-partitions) to the ClientApp inside the container
- Phase 3 will define framework-specific data format expectations

**13. SuperNode Contextualization Parameters**
Include the SuperNode-specific parameter table from research (FLOWER_VERSION, FL_SUPERLINK_ADDRESS, FL_NODE_CONFIG, FL_MAX_RETRIES, FL_MAX_WAIT_TIME, FL_ISOLATION, FL_LOG_LEVEL). Brief descriptions, defaults, and notes on zero-config behavior (empty FL_SUPERLINK_ADDRESS triggers OneGate discovery).

**14. Failure Modes and Error Handling**
- OneGate discovery fails (all retries exhausted): log error, do NOT report ready
- Static address unreachable: container starts but Flower retries connection indefinitely (this is correct behavior)
- Docker daemon not ready: wait loop (same as SuperLink)
- Container UID permission error on /opt/flower/data: `chown 49999:49999` during configure
- Version mismatch with SuperLink: gRPC errors in container logs (document as a debugging hint, not something boot scripts can prevent)

**15. Immutability Model**
- Same as SuperLink: immutable after boot, redeploy to change config

**16. Relationship to SuperLink Spec**
- Brief cross-reference: "The SuperLink appliance spec (Section 01) defines the FL_ENDPOINT publication contract that this appliance's discovery protocol relies on"
- The SuperNode expects the SuperLink to have published FL_READY=YES and FL_ENDPOINT=<ip>:9092 to OneGate before discovery succeeds
- Version compatibility: both appliances MUST run the same Flower version

**Writing style:** Same as SuperLink spec -- technical specification, imperative mood, code blocks for commands, tables for structured data.
  </action>
  <verify>
Verify the file exists and covers all required areas:
- `ls -la /home/pablo/flower-opennebula/spec/02-supernode-appliance.md` (file exists)
- Check the file contains sections for: Discovery Model, Boot Sequence, Docker Configuration, Health Check, Connection Lifecycle, Data Mount, Parameters
- Check the file documents both static and dynamic discovery modes
- Check the file includes the OneGate discovery retry loop specification
- Check the file includes the Docker run command
- Check the file documents failure modes including version mismatch and OneGate failures
  </verify>
  <done>
`spec/02-supernode-appliance.md` exists and an engineer can:
1. Identify every component in the SuperNode QCOW2 image
2. Understand the dual discovery model (static vs OneGate)
3. Trace the complete boot sequence including discovery phase
4. Know exactly how the SuperNode finds and connects to the SuperLink
5. Handle failure scenarios including OneGate failures and version mismatches
  </done>
</task>

</tasks>

<verification>
- The spec section is a standalone reference for the SuperNode appliance
- All APPL-02 requirements are addressed (server connectivity, local data mount points, ML framework selection note)
- The dual discovery model is fully specified with decision logic, retry behavior, and failure handling
- Boot sequence includes the discovery phase (step 4) which is the key differentiator from SuperLink
- Cross-references SuperLink's OneGate publication contract without depending on Plan 01's output (the contract is defined in shared research)
</verification>

<success_criteria>
- `spec/02-supernode-appliance.md` exists with all 16 sections
- An engineer reading only this file (plus the SuperLink cross-reference) can build the SuperNode appliance
- The file covers APPL-02 requirement completely
- Zero-config behavior documented: empty FL_SUPERLINK_ADDRESS triggers OneGate discovery
- Discovery retry loop fully specified (30 retries, 10s interval, 5min total)
- Phase 2+ touchpoints (TLS, GPU, framework variants) marked as placeholders
</success_criteria>

<output>
After completion, create `.planning/phases/01-base-appliance-architecture/01-02-SUMMARY.md`
</output>
