---
phase: 03-ml-framework-variants-and-use-cases
plan: 02
type: execute
wave: 2
depends_on: [03-01]
files_modified: [spec/07-use-case-templates.md]
autonomous: true

must_haves:
  truths:
    - "The spec defines three pre-built use case templates: image classification (PyTorch+ResNet/CNN+CIFAR-10), anomaly detection (scikit-learn+LogisticRegression+tabular), LLM fine-tuning (PyTorch+PEFT/LoRA)"
    - "Each use case template specifies its required contextualization parameters and expected outputs"
    - "A reader can deploy any use case by setting only FL_USE_CASE and ML_FRAMEWORK plus standard FL_* variables -- no SSH, no code changes"
    - "A framework/use-case compatibility matrix prevents invalid combinations at boot"
    - "The spec defines how use case FABs are pre-installed in Docker images and activated at runtime"
  artifacts:
    - path: "spec/07-use-case-templates.md"
      provides: "Complete use case template specification (ML-03)"
      contains: "ML-03"
      min_lines: 250
  key_links:
    - from: "spec/07-use-case-templates.md"
      to: "spec/06-ml-framework-variants.md"
      via: "Use cases require specific framework variants to be deployed"
      pattern: "ML_FRAMEWORK"
    - from: "spec/07-use-case-templates.md"
      to: "spec/03-contextualization-reference.md"
      via: "Defines new FL_USE_CASE variable extending the contextualization reference"
      pattern: "FL_USE_CASE"
    - from: "spec/07-use-case-templates.md"
      to: "spec/02-supernode-appliance.md"
      via: "Use case FABs are pre-installed in SuperNode variant images"
      pattern: "flwr install"
---

<objective>
Write the use case template specification defining three pre-built federated learning use cases (image classification, anomaly detection, LLM fine-tuning) that are deployable purely through contextualization variables.

Purpose: Satisfies ML-03 (pre-built use case templates with contextualization-only deployment). This enables users to deploy working FL training scenarios by setting `FL_USE_CASE` and `ML_FRAMEWORK` without writing any code or SSH-ing into VMs.

Output: `spec/07-use-case-templates.md` -- complete use case template specification
</objective>

<execution_context>
@/home/pablo/.claude/get-shit-done/workflows/execute-plan.md
@/home/pablo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-ml-framework-variants-and-use-cases/03-RESEARCH.md
@.planning/phases/03-ml-framework-variants-and-use-cases/03-01-SUMMARY.md

@spec/00-overview.md
@spec/02-supernode-appliance.md
@spec/03-contextualization-reference.md
@spec/06-ml-framework-variants.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write use case template specification</name>
  <files>spec/07-use-case-templates.md</files>
  <action>
Create `spec/07-use-case-templates.md` as a complete specification section covering ML-03. Follow established spec style from prior sections.

**Document structure (follow this outline):**

1. **Purpose and Scope** -- Define what use case templates are: pre-built Flower App Bundles (FABs) that are pre-installed in the framework-specific Docker images (from spec/06-ml-framework-variants.md) and activated via the `FL_USE_CASE` contextualization variable. Reference ML-03 requirement. State the key constraint: deployment by setting only contextualization variables, no SSH, no code changes.

2. **FL_USE_CASE Contextualization Variable** -- Define the new variable:
   - `FL_USE_CASE = "O|list|Pre-built use case template|none,image-classification,anomaly-detection,llm-fine-tuning|none"`
   - Appliance: SuperNode only
   - Default: `none` (no pre-built use case; user provides their own ClientApp via SuperLink FAB delivery)
   - When set to a value other than `none`, the SuperNode's configure.sh activates the corresponding pre-installed FAB
   - This variable is added to the SuperNode USER_INPUT block

3. **Framework/Use-Case Compatibility Matrix** -- Define which use cases work with which ML_FRAMEWORK values:

   | Use Case | pytorch | tensorflow | sklearn |
   |----------|---------|------------|---------|
   | image-classification | YES (primary) | YES (alternate) | NO |
   | anomaly-detection | NO | NO | YES (primary) |
   | llm-fine-tuning | YES (primary) | NO | NO |
   | none | YES | YES | YES |

   Define boot-time validation: if ML_FRAMEWORK and FL_USE_CASE are incompatible, boot FAILS with a clear error message (e.g., "FL_USE_CASE 'llm-fine-tuning' requires ML_FRAMEWORK 'pytorch', but ML_FRAMEWORK is set to 'sklearn'").

   Provide the configure.sh validation pseudocode as a case statement.

4. **Use Case 1: Image Classification** -- Complete specification:
   - **Identifier:** `image-classification`
   - **Framework:** PyTorch (primary), TensorFlow (alternate implementation)
   - **Model:** Simple CNN for PyTorch variant (or ResNet-18 for advanced); MobileNetV2 for TensorFlow variant
   - **Dataset:** CIFAR-10 via `flwr-datasets` (auto-downloaded on first run; ~170 MB)
   - **FAB structure:** Define pyproject.toml content, client_app.py behavior, server_app.py behavior following Flower quickstart-pytorch pattern
   - **Contextualization parameters for deployment:**
     ```
     ML_FRAMEWORK = pytorch
     FL_USE_CASE = image-classification
     FL_NODE_CONFIG = "partition-id=0 num-partitions=2"
     FL_NUM_ROUNDS = 3
     FL_STRATEGY = FedAvg
     ```
   - **Expected outputs:** Per-round training loss and accuracy, final global model accuracy after FL_NUM_ROUNDS rounds
   - **Data provisioning:** Demo mode uses `flwr-datasets` to auto-download CIFAR-10. Production mode expects pre-provisioned data at `/app/data`. Document both paths.
   - **Resource requirements:** CPU-only, ~2 GB RAM per SuperNode

5. **Use Case 2: Anomaly Detection** -- Complete specification:
   - **Identifier:** `anomaly-detection`
   - **Framework:** scikit-learn
   - **Model:** LogisticRegression with warm_start (federated parameter exchange via coef_/intercept_ extraction)
   - **Dataset:** Tabular data -- demo uses iris dataset via `flwr-datasets` (tiny, ~10 KB). Production mode uses CSV at `/app/data/`.
   - **FAB structure:** Define pyproject.toml content, client_app.py behavior (sklearn parameter extraction pattern), server_app.py behavior
   - **Contextualization parameters:**
     ```
     ML_FRAMEWORK = sklearn
     FL_USE_CASE = anomaly-detection
     FL_NODE_CONFIG = "partition-id=0 num-partitions=4"
     FL_NUM_ROUNDS = 10
     FL_STRATEGY = FedAvg
     ```
   - **Expected outputs:** Per-round accuracy, final global model accuracy
   - **Resource requirements:** CPU-only, ~512 MB RAM per SuperNode (lightest use case)

6. **Use Case 3: LLM Fine-Tuning** -- Complete specification:
   - **Identifier:** `llm-fine-tuning`
   - **Framework:** PyTorch (with bitsandbytes, peft, transformers pre-installed in PyTorch variant)
   - **Model:** Small LLM (e.g., OpenLLaMA 3B) with 4-bit quantization via bitsandbytes and LoRA adapters via peft
   - **Dataset:** Task-specific instruction dataset (not auto-downloadable for demo; requires pre-provisioned data or Hugging Face dataset name in FL_NODE_CONFIG)
   - **FAB structure:** Define pyproject.toml, client_app.py behavior (PEFT LoRA training loop), server_app.py behavior
   - **Contextualization parameters:**
     ```
     ML_FRAMEWORK = pytorch
     FL_USE_CASE = llm-fine-tuning
     FL_NODE_CONFIG = "partition-id=0 num-partitions=2 model-name=openlm-research/open_llama_3b quantization=4bit"
     FL_NUM_ROUNDS = 100
     FL_STRATEGY = FedAvg
     FL_GPU_ENABLED = YES
     ```
   - **CRITICAL constraint:** LLM fine-tuning REQUIRES GPU (Phase 6). Document this as a hard dependency. Without FL_GPU_ENABLED=YES and actual GPU passthrough, this use case will fail with OOM errors or be prohibitively slow. The spec defines the template parameters here; actual deployment requires Phase 6 completion.
   - **VRAM requirements table** (from research):
     | Model | Quantization | VRAM Required |
     |-------|-------------|---------------|
     | 3B | 4-bit | ~10.6 GB |
     | 3B | 8-bit | ~13.5 GB |
     | 7B | 4-bit | ~16.5 GB |
   - **Resource requirements:** GPU with 16+ GB VRAM, ~16 GB system RAM per SuperNode

7. **FAB Pre-installation Process** -- Define how FABs are built and installed into Docker images:
   - Each use case has source code (pyproject.toml + Python files) built into a FAB using `flwr build`
   - FABs are installed into the framework-specific Docker images during Docker build using `flwr install <fab-file> --flwr-dir /app/.flwr`
   - Python dependencies for FABs are already installed in the Docker image (they were added in the Dockerfile from spec/06)
   - The Dockerfiles from spec/06 should include a build stage that copies and installs FABs
   - Provide the Dockerfile addition pattern:
     ```dockerfile
     COPY use-cases/image-classification.fab /tmp/
     RUN flwr install /tmp/image-classification.fab --flwr-dir /app/.flwr && rm /tmp/*.fab
     ```

8. **FAB Activation at Runtime** -- Define how FL_USE_CASE activates a pre-installed FAB:
   - When FL_USE_CASE != "none", configure.sh sets up the environment so the pre-installed FAB is used when SuperLink delivers a run
   - When FL_USE_CASE == "none", the SuperNode waits for FAB delivery from SuperLink (default behavior for custom ClientApps)
   - Document the runtime flow: SuperLink submits a run -> SuperNode receives FAB reference -> if pre-installed, uses local FAB; if not, receives FAB from SuperLink
   - Note: In subprocess mode, the ClientApp code from the FAB runs inside the SuperNode container, so all Python dependencies must be pre-installed in the Docker image

9. **Data Provisioning Strategy** -- Define the two data paths:
   - **Demo mode (default):** Use `flwr-datasets` to auto-download datasets. Requires network access. CIFAR-10 (~170 MB), iris (~10 KB). Document that this breaks the air-gapped boot guarantee from Phase 1, but demo scenarios are expected to have network access.
   - **Production mode:** Data pre-provisioned at `/opt/flower/data/` on the host (mounted read-only as `/app/data:ro` in the container, per Phase 1 spec). The ClientApp reads from `/app/data/` instead of downloading. Selection between demo/production mode is automatic: if `/app/data/` contains files, use them; otherwise, fall back to `flwr-datasets` download.
   - Note the data mount path from Phase 1 spec: `/opt/flower/data -> /app/data:ro`

10. **New Contextualization Variables Summary** -- Summary table of all new variables introduced in Phase 3:

    | Variable | USER_INPUT | Appliance | Default | Phase |
    |----------|-----------|-----------|---------|-------|
    | FL_USE_CASE | `O\|list\|Pre-built use case template\|none,image-classification,anomaly-detection,llm-fine-tuning\|none` | SuperNode | none | 3 |

    Note: ML_FRAMEWORK was already defined as a Phase 3 placeholder in spec/03-contextualization-reference.md. It becomes functional with this spec section.

**Style requirements:**
- Match established spec style (numbered sections with `###`, tables, code blocks, pseudocode)
- Include "Requirement: ML-03" and "Phase: 03" header
- Include code examples for each use case's client_app.py showing the Flower ClientApp pattern (simplified from research examples)
- End with spec footer
  </action>
  <verify>
1. File exists: `ls spec/07-use-case-templates.md`
2. Contains all three use cases: grep for "image-classification", "anomaly-detection", "llm-fine-tuning"
3. Contains FL_USE_CASE variable: grep for "FL_USE_CASE"
4. Contains compatibility matrix: grep for "Compatibility"
5. Contains FAB installation pattern: grep for "flwr install"
6. Contains data provisioning: grep for "Data Provisioning" or "demo mode"
7. References ML-03: grep for "ML-03"
8. Document is substantive: `wc -l spec/07-use-case-templates.md` should be >250 lines
  </verify>
  <done>
spec/07-use-case-templates.md exists with all 10 sections, defines three use case templates with complete contextualization parameters and expected outputs, includes a compatibility matrix with boot-time validation, defines FAB pre-installation and activation patterns, and covers data provisioning strategy. A reader can deploy any use case by setting only FL_USE_CASE and ML_FRAMEWORK plus standard FL_* variables.
  </done>
</task>

</tasks>

<verification>
- spec/07-use-case-templates.md exists and covers ML-03
- Three use case templates fully specified with contextualization parameters and expected outputs
- FL_USE_CASE variable defined with USER_INPUT format
- Compatibility matrix prevents invalid ML_FRAMEWORK + FL_USE_CASE combinations
- FAB pre-installation and runtime activation flows defined
- Data provisioning covers both demo (auto-download) and production (pre-provisioned) paths
- LLM fine-tuning correctly identifies GPU as a hard dependency (Phase 6)
- Document follows established spec style
</verification>

<success_criteria>
1. A reader can deploy image classification by setting ML_FRAMEWORK=pytorch, FL_USE_CASE=image-classification, and standard FL_* variables
2. A reader can deploy anomaly detection by setting ML_FRAMEWORK=sklearn, FL_USE_CASE=anomaly-detection
3. A reader understands that LLM fine-tuning requires GPU (Phase 6) and what contextualization variables to set
4. A reader can trace the FAB lifecycle from build to pre-installation to runtime activation
5. The compatibility matrix prevents every invalid framework/use-case combination
</success_criteria>

<output>
After completion, create `.planning/phases/03-ml-framework-variants-and-use-cases/03-02-SUMMARY.md`
</output>
